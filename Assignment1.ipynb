{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark-2.3.2-bin-hadoop2.7/python (2.3.2)\r\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/alumno/miniconda3/envs/py37/lib/python3.7/site-packages (from pyspark) (0.10.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /home/alumno/miniconda3/envs/py37/lib/python3.7/site-packages (0.8.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from tabulate import tabulate\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[*]\", \"First App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"/home/alumno/ML-serialized-parallelized/Datasets/spam.data\")\n",
    "records_Xy = text_file.map(lambda x: x.split(\" \"))\\\n",
    "            .map(lambda x: [float(z) for z in x])\\\n",
    "            .map(lambda x: (np.array(x[:57]), x[57]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(records_Xy):\n",
    "  records_count = float(records_Xy.count())\n",
    "  records_mean = records_Xy.map(lambda record: record[0]).reduce(lambda x, y: x+y)/records_count\n",
    "  records_std = records_Xy.map(lambda record: np.power(record[0]-records_mean,2))\\\n",
    "                          .reduce(lambda record_1, record_2: record_1+record_2)\n",
    "  return records_Xy.map(lambda record: ((record[0]-records_mean)/np.sqrt(records_std/records_count),record[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter(records_Xy, train_ratio):\n",
    "  if train_ratio>0.9:\n",
    "    raise Exception('''Sorry, you are setting very small ratio for test set and it's not acceptable\n",
    "                    please choose a train_ratio less than or equal 0.9''')\n",
    "  records_Xy = records_Xy.repartition(int(train_ratio*20))\n",
    "  record_index = records_Xy.zipWithIndex()\n",
    "  splitting_index = int(records_Xy.count() * train_ratio)\n",
    "  train_set = record_index.filter(lambda record: record[1] <= splitting_index).map(lambda record: record[0])\n",
    "  test_set = record_index.filter(lambda record: record[1] > splitting_index).map(lambda record: record[0])\n",
    "  return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(records_X1y, Wb):\n",
    "    # returns records_X1y_yhat\n",
    "    return records_X1y.map(lambda record: (record[0], record[1], record[0].dot(Wb)))\\\n",
    "                      .map(lambda record: (record[0], record[1], 1 / (1 + np.exp(-record[2]) )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(records_X1y_yhat, Wb, lambda_reg):\n",
    "  epsilon = 1e-5\n",
    "  records_count = records_X1y_yhat.count()\n",
    "  cost = records_X1y_yhat.map(lambda record: record[1]*math.log(record[2] + epsilon) + (1-record[1])*math.log(1-record[2] + epsilon))\\\n",
    "                        .reduce(lambda record_1, record_2: record_1+record_2)\n",
    "  return (-1/records_count)*cost + lambda_reg/(2*records_count)*np.sum(np.power(Wb,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(record_Xy, iterations, learning_rate, lambda_reg, print_cost_per_iteration = False):\n",
    "  records_count = record_Xy.count()\n",
    "  features_counts = len(record_Xy.take(1)[0][0])\n",
    "  np.random.seed(123)\n",
    "  Wb = np.random.random([features_counts+1,])\n",
    "  loss = []\n",
    "  records_X1y = record_Xy.map(lambda record: (np.append(record[0], 1), record[1]))\n",
    "  for i in range(iterations):\n",
    "    records_X1y_yhat = sigmoid(records_X1y, Wb)\n",
    "    dw_init = records_X1y_yhat.map(lambda record: record[0]*(record[2]-record[1]))\\\n",
    "                         .reduce(lambda record_1, record_2 : np.subtract(record_1, record_2))\n",
    "    dw = np.add(dw_init, lambda_reg*Wb)/records_count\n",
    "    Wb = np.subtract(Wb, learning_rate*dw)\n",
    "    cost = cost_function(records_X1y_yhat, Wb, lambda_reg)\n",
    "    loss.append(cost)\n",
    "    if print_cost_per_iteration:\n",
    "      print(f'Iteratoin {i} : cost is : {cost}')\n",
    "    \n",
    "  return Wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(record_Xy, Wb):\n",
    "  records_X1y = record_Xy.map(lambda record: (np.append(record[0], 1), record[1]))\n",
    "  return sigmoid(records_X1y, Wb).map(lambda record :  (record[1], 1 if record[2] > 0.5 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(record_Xy, Wb):\n",
    "    def MapToBinaryState(record):\n",
    "      if record[0] == 0 and record[1] == 0:\n",
    "          return ('true_negative', 1)\n",
    "      elif record[0] == 1 and record[1] == 0:\n",
    "        return ('false_negative', 1)\n",
    "      elif record[0] == 1 and record[1] == 1:\n",
    "        return ('true_positive', 1)\n",
    "      else:\n",
    "        return ('false_positive', 1)\n",
    "    records_count = record_Xy.count()\n",
    "    record_y_yhat = predict(record_Xy, Wb)\n",
    "    binary_states = record_y_yhat.map(MapToBinaryState).reduceByKey(lambda record_1, record_2: record_1+record_2).collect()\n",
    "    binary_state_dictionary = dict(binary_states)\n",
    "    precision = binary_state_dictionary.get('true_positive',0) / (binary_state_dictionary.get('true_positive',0) + binary_state_dictionary.get('false_positive',0))\n",
    "    recall =  binary_state_dictionary.get('true_positive',0) / (binary_state_dictionary.get('false_negative',0) + binary_state_dictionary.get('true_positive',0))\n",
    "    accuracy = (binary_state_dictionary.get('true_positive',0) + binary_state_dictionary.get('true_negative',0))/records_count\n",
    "    table = [\n",
    "              [\"Precision\", precision],\n",
    "              [\"Recall\", recall],\n",
    "              [\"Accuracy\", accuracy]\n",
    "            ]\n",
    "    print(tabulate(table))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alumno/ML-serialized-parallelized'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  --------\n",
      "Precision  0.881503\n",
      "Recall     0.847222\n",
      "Accuracy   0.895652\n",
      "---------  --------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8956521739130435"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_Xy = normalize(records_Xy)\n",
    "train_set, test_set = train_test_splitter(records_Xy, 0.8)\n",
    "Wb = train(train_set, 50, 0.5, 10)\n",
    "accuracy(test_set, Wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  --------\n",
      "Precision  0.761905\n",
      "Recall     0.8\n",
      "Accuracy   0.898876\n",
      "---------  --------\n",
      "---------  --------\n",
      "Precision  0.8\n",
      "Recall     0.8\n",
      "Accuracy   0.912088\n",
      "---------  --------\n",
      "---------  --------\n",
      "Precision  0.625\n",
      "Recall     1\n",
      "Accuracy   0.868132\n",
      "---------  --------\n",
      "---------  --------\n",
      "Precision  0.727273\n",
      "Recall     0.8\n",
      "Accuracy   0.892473\n",
      "---------  --------\n",
      "---------  --------\n",
      "Precision  0.619048\n",
      "Recall     0.65\n",
      "Accuracy   0.83871\n",
      "---------  --------\n",
      "---------  --------\n",
      "Precision  0.761905\n",
      "Recall     0.8\n",
      "Accuracy   0.901099\n",
      "---------  --------\n",
      "---------  --------\n",
      "Precision  0.708333\n",
      "Recall     0.85\n",
      "Accuracy   0.89011\n",
      "---------  --------\n",
      "---------  --------\n",
      "Precision  0.6\n",
      "Recall     0.6\n",
      "Accuracy   0.824176\n",
      "---------  --------\n",
      "---------  --------\n",
      "Precision  0.9375\n",
      "Recall     0.75\n",
      "Accuracy   0.934066\n",
      "---------  --------\n",
      "---------  --------\n",
      "Precision  0.68\n",
      "Recall     0.85\n",
      "Accuracy   0.877778\n",
      "---------  --------\n"
     ]
    }
   ],
   "source": [
    "records_Xy = normalize(records_Xy)\n",
    "cross_validation_accuray = []\n",
    "\n",
    "def BlockHandler(block):\n",
    "  block_rdd = sc.parallelize(block)\n",
    "  train_set, test_set = train_test_splitter(block_rdd, 0.8)\n",
    "  Wb = train(train_set, 50, 0.5, 10)\n",
    "  cross_validation_accuray.append(accuracy(test_set, Wb))\n",
    "\n",
    "\n",
    "glomed_records_Xy = records_Xy.repartition(10).glom().collect()\n",
    "for record in glomed_records_Xy:\n",
    "  BlockHandler(record)\n",
    "# acc.take(1)\n",
    "\n",
    "# print(glomed_records_Xy.take(1))\n",
    "# accuracy = records_Xy.mapPartitions(BlockHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
